{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Butterfly Images - Pytorch\n",
    "\n",
    "In June 2016, I read an article about tensorflow practiced on butterfly images. Just right after presenting my Master's capstone project which I changed from Convulutional Neural Networks to Enterprise Architecture. I didn't know Neural Networks were the building block of Deep Learning or the blog I read is about Deep Learning and Computer Vision. This article triggered to start my learning journey on Deep Learning, Computer Vision and Artificial Intelligence. You can find the blog post here:  [a poet does tensorflow](https://www.oreilly.com/learning/a-poet-does-tensorflow) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dataset = datasets.ImageFolder('path/to/data', transform=transform)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of data set to use as test\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "data_set = datasets.ImageFolder(root=\"data\",transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=4,shuffle=True,num_workers=2)\n",
    "\n",
    "# obtain training indices that will be used for test\n",
    "num_data = len(data_set)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * num_data))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler  = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "trainloader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
    "                                           sampler = train_sampler, num_workers=num_workers)\n",
    "testloader  = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "\n",
    "classes = ('blurry','clear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Images\n",
    "Here we are defining our network architecture and training them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0611225727832678\n",
      "Training loss: 0.6043970593900392\n",
      "Training loss: 0.5429921357920675\n",
      "Training loss: 0.5433650233528831\n",
      "Training loss: 0.5538575423486305\n",
      "Training loss: 0.5491892677364927\n",
      "Training loss: 0.5382337434725328\n",
      "Training loss: 0.5180408950104858\n",
      "Training loss: 0.5232874136982542\n",
      "Training loss: 0.49697643518447876\n",
      "Training loss: 0.4888196596593568\n",
      "Training loss: 0.5189437491424156\n",
      "Training loss: 0.48548668442350446\n",
      "Training loss: 0.507999472545855\n",
      "Training loss: 0.49990487121271365\n",
      "Training loss: 0.4926995291854396\n",
      "Training loss: 0.5013617800943779\n",
      "Training loss: 0.4933172852704019\n",
      "Training loss: 0.500824690768213\n",
      "Training loss: 0.48462424359538336\n",
      "Training loss: 0.4833958266359387\n",
      "Training loss: 0.47013867939963483\n",
      "Training loss: 0.49537186911611847\n",
      "Training loss: 0.49338152462785895\n",
      "Training loss: 0.4898672717990297\n",
      "Training loss: 0.49308923157778656\n",
      "Training loss: 0.5086138898676092\n",
      "Training loss: 0.4916872644063198\n",
      "Training loss: 0.4727043920394146\n",
      "Training loss: 0.47469859954082605\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(150528, 500),\n",
    "                      nn.Linear(500, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten images into a long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '1.weight', '1.bias', '3.weight', '3.bias', '5.weight', '5.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For  transfer learning let's use one of the most famous pre-trained models:resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model_resnet152 = models.resnet152(pretrained=True)\n",
    "model_resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model_resnet152.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model_resnet152.classifier = classifier\n",
    "\n",
    "optimizer = optim.SGD(model_resnet152.parameters(), lr=0.003)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30.. Train loss: 0.960.. Test loss: 1.205.. Test accuracy: 0.000\n",
      "Epoch 1/30.. Train loss: 0.789.. Test loss: 1.365.. Test accuracy: 0.000\n",
      "Epoch 1/30.. Train loss: 0.834.. Test loss: 1.181.. Test accuracy: 0.006\n",
      "Epoch 1/30.. Train loss: 0.733.. Test loss: 1.362.. Test accuracy: 0.000\n",
      "Epoch 1/30.. Train loss: 0.857.. Test loss: 1.361.. Test accuracy: 0.000\n",
      "Epoch 1/30.. Train loss: 0.753.. Test loss: 1.306.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.938.. Test loss: 1.066.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.818.. Test loss: 1.023.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.824.. Test loss: 0.983.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.877.. Test loss: 1.005.. Test accuracy: 0.006\n",
      "Epoch 2/30.. Train loss: 0.887.. Test loss: 1.188.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.867.. Test loss: 1.137.. Test accuracy: 0.000\n",
      "Epoch 2/30.. Train loss: 0.790.. Test loss: 1.131.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.703.. Test loss: 1.234.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.743.. Test loss: 0.821.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.916.. Test loss: 0.904.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.970.. Test loss: 0.855.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.940.. Test loss: 0.813.. Test accuracy: 0.000\n",
      "Epoch 3/30.. Train loss: 0.718.. Test loss: 1.302.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.893.. Test loss: 1.294.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.921.. Test loss: 1.112.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.962.. Test loss: 0.913.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.803.. Test loss: 1.032.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.803.. Test loss: 0.921.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.917.. Test loss: 1.425.. Test accuracy: 0.000\n",
      "Epoch 4/30.. Train loss: 0.766.. Test loss: 1.089.. Test accuracy: 0.006\n",
      "Epoch 5/30.. Train loss: 0.767.. Test loss: 1.073.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.793.. Test loss: 0.994.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.681.. Test loss: 1.097.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.854.. Test loss: 1.262.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.822.. Test loss: 1.398.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.927.. Test loss: 1.121.. Test accuracy: 0.000\n",
      "Epoch 5/30.. Train loss: 0.855.. Test loss: 0.888.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 0.808.. Test loss: 1.298.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 0.762.. Test loss: 1.285.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 1.013.. Test loss: 1.082.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 0.837.. Test loss: 1.382.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 0.906.. Test loss: 1.045.. Test accuracy: 0.000\n",
      "Epoch 6/30.. Train loss: 0.775.. Test loss: 1.393.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.801.. Test loss: 1.081.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.725.. Test loss: 1.230.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.778.. Test loss: 1.409.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.788.. Test loss: 1.426.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.784.. Test loss: 1.400.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.907.. Test loss: 0.549.. Test accuracy: 0.000\n",
      "Epoch 7/30.. Train loss: 0.895.. Test loss: 0.969.. Test accuracy: 0.000\n",
      "Epoch 8/30.. Train loss: 0.726.. Test loss: 1.357.. Test accuracy: 0.006\n",
      "Epoch 8/30.. Train loss: 0.790.. Test loss: 1.193.. Test accuracy: 0.000\n",
      "Epoch 8/30.. Train loss: 0.896.. Test loss: 1.032.. Test accuracy: 0.000\n",
      "Epoch 8/30.. Train loss: 0.828.. Test loss: 1.149.. Test accuracy: 0.000\n",
      "Epoch 8/30.. Train loss: 0.788.. Test loss: 0.995.. Test accuracy: 0.000\n",
      "Epoch 8/30.. Train loss: 0.879.. Test loss: 1.486.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.758.. Test loss: 1.254.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.841.. Test loss: 0.797.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.885.. Test loss: 0.830.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.900.. Test loss: 1.278.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.720.. Test loss: 1.093.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.793.. Test loss: 1.120.. Test accuracy: 0.000\n",
      "Epoch 9/30.. Train loss: 0.902.. Test loss: 1.272.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.732.. Test loss: 1.106.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.886.. Test loss: 1.035.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.794.. Test loss: 0.518.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.818.. Test loss: 0.988.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.768.. Test loss: 1.169.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.924.. Test loss: 1.163.. Test accuracy: 0.000\n",
      "Epoch 10/30.. Train loss: 0.914.. Test loss: 1.166.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.888.. Test loss: 1.071.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.863.. Test loss: 1.245.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.699.. Test loss: 1.321.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.940.. Test loss: 0.994.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.835.. Test loss: 1.198.. Test accuracy: 0.000\n",
      "Epoch 11/30.. Train loss: 0.801.. Test loss: 0.434.. Test accuracy: 0.011\n",
      "Epoch 12/30.. Train loss: 0.936.. Test loss: 0.941.. Test accuracy: 0.000\n",
      "Epoch 12/30.. Train loss: 0.753.. Test loss: 0.760.. Test accuracy: 0.000\n",
      "Epoch 12/30.. Train loss: 0.843.. Test loss: 0.932.. Test accuracy: 0.000\n",
      "Epoch 12/30.. Train loss: 1.014.. Test loss: 1.247.. Test accuracy: 0.000\n",
      "Epoch 12/30.. Train loss: 0.933.. Test loss: 0.758.. Test accuracy: 0.000\n",
      "Epoch 12/30.. Train loss: 0.741.. Test loss: 0.781.. Test accuracy: 0.006\n",
      "Epoch 12/30.. Train loss: 0.798.. Test loss: 0.872.. Test accuracy: 0.000\n",
      "Epoch 13/30.. Train loss: 0.850.. Test loss: 1.152.. Test accuracy: 0.000\n",
      "Epoch 13/30.. Train loss: 0.949.. Test loss: 0.500.. Test accuracy: 0.000\n",
      "Epoch 13/30.. Train loss: 0.839.. Test loss: 0.927.. Test accuracy: 0.000\n",
      "Epoch 13/30.. Train loss: 0.821.. Test loss: 1.513.. Test accuracy: 0.000\n",
      "Epoch 13/30.. Train loss: 0.769.. Test loss: 0.857.. Test accuracy: 0.011\n",
      "Epoch 13/30.. Train loss: 0.835.. Test loss: 1.055.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.922.. Test loss: 1.209.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.998.. Test loss: 1.256.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.840.. Test loss: 1.298.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.818.. Test loss: 1.402.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.774.. Test loss: 1.006.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.960.. Test loss: 1.199.. Test accuracy: 0.000\n",
      "Epoch 14/30.. Train loss: 0.788.. Test loss: 0.976.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.824.. Test loss: 1.348.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.771.. Test loss: 1.006.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.716.. Test loss: 1.218.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.781.. Test loss: 0.862.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.881.. Test loss: 1.042.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.935.. Test loss: 0.846.. Test accuracy: 0.000\n",
      "Epoch 15/30.. Train loss: 0.808.. Test loss: 1.042.. Test accuracy: 0.000\n",
      "Epoch 16/30.. Train loss: 0.993.. Test loss: 0.887.. Test accuracy: 0.000\n",
      "Epoch 16/30.. Train loss: 0.916.. Test loss: 1.352.. Test accuracy: 0.000\n",
      "Epoch 16/30.. Train loss: 0.875.. Test loss: 0.565.. Test accuracy: 0.000\n",
      "Epoch 16/30.. Train loss: 0.814.. Test loss: 0.689.. Test accuracy: 0.006\n",
      "Epoch 16/30.. Train loss: 0.915.. Test loss: 1.411.. Test accuracy: 0.000\n",
      "Epoch 16/30.. Train loss: 0.701.. Test loss: 0.850.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.881.. Test loss: 1.088.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.944.. Test loss: 0.798.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.757.. Test loss: 0.756.. Test accuracy: 0.006\n",
      "Epoch 17/30.. Train loss: 0.902.. Test loss: 0.745.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.826.. Test loss: 1.072.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.793.. Test loss: 1.222.. Test accuracy: 0.000\n",
      "Epoch 17/30.. Train loss: 0.851.. Test loss: 1.128.. Test accuracy: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30.. Train loss: 0.713.. Test loss: 0.589.. Test accuracy: 0.000\n",
      "Epoch 18/30.. Train loss: 1.014.. Test loss: 0.826.. Test accuracy: 0.000\n",
      "Epoch 18/30.. Train loss: 1.011.. Test loss: 1.098.. Test accuracy: 0.000\n",
      "Epoch 18/30.. Train loss: 0.733.. Test loss: 1.167.. Test accuracy: 0.000\n",
      "Epoch 18/30.. Train loss: 0.743.. Test loss: 0.981.. Test accuracy: 0.000\n",
      "Epoch 18/30.. Train loss: 0.809.. Test loss: 1.083.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.733.. Test loss: 1.435.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.927.. Test loss: 0.933.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.813.. Test loss: 1.116.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.829.. Test loss: 0.970.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.775.. Test loss: 1.418.. Test accuracy: 0.006\n",
      "Epoch 19/30.. Train loss: 0.970.. Test loss: 0.532.. Test accuracy: 0.000\n",
      "Epoch 19/30.. Train loss: 0.715.. Test loss: 0.716.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.754.. Test loss: 1.066.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.933.. Test loss: 1.322.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.811.. Test loss: 1.285.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.879.. Test loss: 1.309.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.706.. Test loss: 1.131.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.935.. Test loss: 1.111.. Test accuracy: 0.000\n",
      "Epoch 20/30.. Train loss: 0.891.. Test loss: 0.983.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.946.. Test loss: 1.117.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.832.. Test loss: 1.121.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.849.. Test loss: 0.818.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.879.. Test loss: 1.013.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.847.. Test loss: 1.277.. Test accuracy: 0.000\n",
      "Epoch 21/30.. Train loss: 0.763.. Test loss: 1.189.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.776.. Test loss: 1.455.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.871.. Test loss: 1.179.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.714.. Test loss: 1.246.. Test accuracy: 0.006\n",
      "Epoch 22/30.. Train loss: 0.891.. Test loss: 1.150.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.751.. Test loss: 1.018.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.820.. Test loss: 1.233.. Test accuracy: 0.000\n",
      "Epoch 22/30.. Train loss: 0.822.. Test loss: 0.868.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.730.. Test loss: 1.133.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.830.. Test loss: 1.390.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.849.. Test loss: 0.806.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.793.. Test loss: 0.978.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.918.. Test loss: 0.927.. Test accuracy: 0.000\n",
      "Epoch 23/30.. Train loss: 0.864.. Test loss: 0.957.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.758.. Test loss: 0.910.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.922.. Test loss: 1.019.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.927.. Test loss: 1.178.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.866.. Test loss: 1.006.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 1.028.. Test loss: 1.041.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.884.. Test loss: 1.396.. Test accuracy: 0.000\n",
      "Epoch 24/30.. Train loss: 0.718.. Test loss: 1.120.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.917.. Test loss: 0.986.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.850.. Test loss: 1.413.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.914.. Test loss: 1.252.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.899.. Test loss: 1.288.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.705.. Test loss: 0.876.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.751.. Test loss: 0.764.. Test accuracy: 0.000\n",
      "Epoch 25/30.. Train loss: 0.764.. Test loss: 1.286.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.861.. Test loss: 0.780.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.678.. Test loss: 1.174.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.968.. Test loss: 1.256.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.812.. Test loss: 1.580.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.761.. Test loss: 1.241.. Test accuracy: 0.000\n",
      "Epoch 26/30.. Train loss: 0.854.. Test loss: 1.130.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.789.. Test loss: 1.315.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.957.. Test loss: 1.519.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.766.. Test loss: 1.493.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.782.. Test loss: 1.303.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.749.. Test loss: 1.491.. Test accuracy: 0.006\n",
      "Epoch 27/30.. Train loss: 0.938.. Test loss: 1.041.. Test accuracy: 0.000\n",
      "Epoch 27/30.. Train loss: 0.833.. Test loss: 0.800.. Test accuracy: 0.000\n",
      "Epoch 28/30.. Train loss: 0.709.. Test loss: 1.374.. Test accuracy: 0.000\n",
      "Epoch 28/30.. Train loss: 0.880.. Test loss: 1.190.. Test accuracy: 0.000\n",
      "Epoch 28/30.. Train loss: 0.744.. Test loss: 0.912.. Test accuracy: 0.006\n",
      "Epoch 28/30.. Train loss: 0.818.. Test loss: 1.178.. Test accuracy: 0.000\n",
      "Epoch 28/30.. Train loss: 0.962.. Test loss: 1.287.. Test accuracy: 0.000\n",
      "Epoch 28/30.. Train loss: 0.792.. Test loss: 1.396.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.762.. Test loss: 1.075.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.776.. Test loss: 0.729.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.842.. Test loss: 1.318.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.864.. Test loss: 0.865.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 1.035.. Test loss: 0.855.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.808.. Test loss: 0.546.. Test accuracy: 0.000\n",
      "Epoch 29/30.. Train loss: 0.816.. Test loss: 1.304.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.732.. Test loss: 1.145.. Test accuracy: 0.006\n",
      "Epoch 30/30.. Train loss: 0.837.. Test loss: 1.259.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.728.. Test loss: 1.206.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.880.. Test loss: 0.817.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.813.. Test loss: 0.933.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.900.. Test loss: 1.022.. Test accuracy: 0.000\n",
      "Epoch 30/30.. Train loss: 0.855.. Test loss: 1.461.. Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "device = 'cpu'\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model_resnet152.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model_resnet152.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model_resnet152.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model_resnet152.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_resnet152.state_dict(), 'resnet_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
