{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latter Classification Neural Network\n",
    "\n",
    "Trying to build the best CNN architecture for the images we have regarding to this paper : https://arxiv.org/pdf/1606.02228.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Selections\n",
    "In order to use state of the art toolset for this research paper we installed CUDA 10 , Pytorch 1.0, Python 3.7 and openCV 3.4 on ubuntu 16.04 with NVDIA 1080 GPU.  https://arxiv.org/pdf/1606.02228.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for this section\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "# Use GPU if it's available\n",
    "from collections import OrderedDict\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of data set to use as test\n",
    "validation_size = 0.5\n",
    "test_validation_size = 0.4\n",
    "\n",
    "transform = transforms.Compose([ transforms.CenterCrop(1000), transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "data_set = dset.ImageFolder(root=\"data\",transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=4,shuffle=True,num_workers=2)\n",
    "\n",
    "# obtain training indices that will be used for test\n",
    "num_data = len(data_set)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_validation_size * num_data))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "num_train_data = len(test_idx)\n",
    "split_validation = int(np.floor(validation_size * num_train_data))\n",
    "test_idx, validation_idx = test_idx[split_validation:], test_idx[:split_validation]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler  = SubsetRandomSampler(validation_idx)\n",
    "test_sampler  = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
    "                                           sampler = train_sampler, num_workers=num_workers)\n",
    "validation_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "test_loader  = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "\n",
    "classes = ('blurry','clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerCNN(torch.nn.Module):\n",
    "  \n",
    "    #Our batch shape for input x is (3, 224, 224)\n",
    "    #RELU with batchnorm.\n",
    "    def __init__(self):\n",
    "        super(MultilayerCNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(8 * 7 *7, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(-1, 8 * 7 *7)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply linear learning rate decay \n",
    "def adjust_lr(init_lr, optimizer, epoch, n_epochs):\n",
    "    lr = init_lr * (1 - (epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLossAndOptimizer(net, learning_rate=0.001):  \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss() \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)   \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        #Create our loss and optimizer functions\n",
    "        loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "        learning_rate = adjust_lr(learning_rate, optimizer, epoch, n_epochs)\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in validation_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.item()\n",
    "        \n",
    "        total_test_loss = 0\n",
    "        accuracy = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "            \n",
    "                #Wrap tensors in Variables\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "                #Forward pass\n",
    "                test_outputs = net(inputs)\n",
    "                test_loss_size = loss(test_outputs, labels)\n",
    "                total_test_loss += test_loss_size.item()\n",
    "                \n",
    "                ps = torch.exp(test_outputs)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                \n",
    "        net.train()\n",
    "        \n",
    "        print(\"Training loss = {:.2f}\".format(total_train_loss / len(train_loader)))\n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(validation_loader)))\n",
    "        print(\"Test loss = {:.2f}\".format(total_test_loss / len(test_loader)))\n",
    "        print(\"Test Accuracy = {:.2f}\".format(accuracy / len(test_loader)))\n",
    "           \n",
    "        with open('multilayer_cnn.csv','a') as f:\n",
    "            f.write(f\"Train loss: {total_train_loss:.3f}.. \"\n",
    "                    f\"Validation loss: {total_val_loss:.3f}.. \"\n",
    "                    f\"Test loss: {total_test_loss/len(test_loader):.3f}.. \"\n",
    "                    f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 128\n",
      "epochs= 5\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 12% \t train_loss: 0.99 took: 26.33s\n",
      "Epoch 1, 24% \t train_loss: 0.93 took: 25.32s\n",
      "Epoch 1, 36% \t train_loss: 0.80 took: 25.19s\n",
      "Epoch 1, 48% \t train_loss: 0.76 took: 25.14s\n",
      "Epoch 1, 60% \t train_loss: 0.90 took: 25.13s\n",
      "Epoch 1, 72% \t train_loss: 0.77 took: 25.28s\n",
      "Epoch 1, 84% \t train_loss: 0.77 took: 24.95s\n",
      "Epoch 1, 96% \t train_loss: 0.90 took: 25.46s\n",
      "Training loss = 0.60\n",
      "Validation loss = 0.66\n",
      "Test loss = 0.63\n",
      "Test Accuracy = 0.61\n",
      "Epoch 2, 12% \t train_loss: 0.76 took: 25.93s\n",
      "Epoch 2, 24% \t train_loss: 0.73 took: 26.49s\n",
      "Epoch 2, 36% \t train_loss: 0.86 took: 26.26s\n",
      "Epoch 2, 48% \t train_loss: 0.88 took: 26.13s\n",
      "Epoch 2, 60% \t train_loss: 0.70 took: 25.78s\n",
      "Epoch 2, 72% \t train_loss: 0.83 took: 26.17s\n",
      "Epoch 2, 84% \t train_loss: 0.70 took: 25.69s\n",
      "Epoch 2, 96% \t train_loss: 0.80 took: 25.96s\n",
      "Training loss = 0.51\n",
      "Validation loss = 0.63\n",
      "Test loss = 0.55\n",
      "Test Accuracy = 0.71\n",
      "Epoch 3, 12% \t train_loss: 0.90 took: 26.01s\n",
      "Epoch 3, 24% \t train_loss: 0.84 took: 26.15s\n",
      "Epoch 3, 36% \t train_loss: 0.72 took: 25.98s\n",
      "Epoch 3, 48% \t train_loss: 0.86 took: 26.10s\n",
      "Epoch 3, 60% \t train_loss: 0.72 took: 26.26s\n",
      "Epoch 3, 72% \t train_loss: 0.74 took: 26.31s\n",
      "Epoch 3, 84% \t train_loss: 0.78 took: 25.78s\n",
      "Epoch 3, 96% \t train_loss: 0.93 took: 25.77s\n",
      "Training loss = 0.53\n",
      "Validation loss = 0.55\n",
      "Test loss = 0.53\n",
      "Test Accuracy = 0.72\n",
      "Epoch 4, 12% \t train_loss: 0.84 took: 27.35s\n",
      "Epoch 4, 24% \t train_loss: 0.73 took: 25.80s\n",
      "Epoch 4, 36% \t train_loss: 0.67 took: 25.48s\n",
      "Epoch 4, 48% \t train_loss: 0.80 took: 25.62s\n",
      "Epoch 4, 60% \t train_loss: 0.66 took: 25.78s\n",
      "Epoch 4, 72% \t train_loss: 0.82 took: 25.84s\n",
      "Epoch 4, 84% \t train_loss: 0.83 took: 25.73s\n",
      "Epoch 4, 96% \t train_loss: 0.80 took: 25.66s\n",
      "Training loss = 0.51\n",
      "Validation loss = 0.58\n",
      "Test loss = 0.70\n",
      "Test Accuracy = 0.63\n",
      "Epoch 5, 12% \t train_loss: 0.78 took: 26.78s\n",
      "Epoch 5, 24% \t train_loss: 0.93 took: 26.14s\n",
      "Epoch 5, 36% \t train_loss: 0.79 took: 26.06s\n",
      "Epoch 5, 48% \t train_loss: 0.68 took: 25.67s\n",
      "Epoch 5, 60% \t train_loss: 0.81 took: 26.03s\n",
      "Epoch 5, 72% \t train_loss: 0.80 took: 26.18s\n",
      "Epoch 5, 84% \t train_loss: 0.95 took: 26.20s\n",
      "Epoch 5, 96% \t train_loss: 0.89 took: 26.09s\n",
      "Training loss = 0.54\n",
      "Validation loss = 0.53\n",
      "Test loss = 0.65\n",
      "Test Accuracy = 0.61\n",
      "Training finished, took 1547.41s\n"
     ]
    }
   ],
   "source": [
    "CNN = MultilayerCNN()\n",
    "trainNet(CNN, batch_size=128, n_epochs=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
